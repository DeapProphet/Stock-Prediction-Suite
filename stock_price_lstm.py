# -*- coding: utf-8 -*-
"""Stock_Price_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q5-Y4WHCrhylIK9bmcXVOYGZs3jOhKdz
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import io
import tensorflow as tf
from google.colab import files
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# ---------------------------------------------------------
# 1. UPLOAD
# ---------------------------------------------------------
print("Upload your 6 bank files (PKR data)...")
uploaded = files.upload()
filenames = list(uploaded.keys())

if not filenames:
    raise ValueError("No files uploaded.")

# ---------------------------------------------------------
# 2. CONFIGURATION
# ---------------------------------------------------------
LOOKBACK = 60       # Past 60 days of history
EPOCHS = 100        # Max epochs (EarlyStopping will cut this short)
BATCH_SIZE = 32

rows = len(filenames)
fig, axes = plt.subplots(rows, 2, figsize=(16, 5 * rows))
if rows == 1: axes = axes.reshape(1, -1)

print(f"\nInitializing LSTM Protocol on {rows} datasets...")

# ---------------------------------------------------------
# 3. HELPER: SLIDING WINDOW
# ---------------------------------------------------------
def create_sequences(dataset, look_back=60):
    X, y = [], []
    for i in range(len(dataset) - look_back):
        X.append(dataset[i:(i + look_back), 0])
        y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(y)

# ---------------------------------------------------------
# 4. MAIN LOOP
# ---------------------------------------------------------
for i, filename in enumerate(filenames):
    print(f"\nTraining on {filename}...")

    # --- A. LOAD & CLEAN ---
    try:
        df = pd.read_csv(io.BytesIO(uploaded[filename]))
    except:
        print(f"Failed to read {filename}")
        continue

    # Robust Column Finder
    possible_cols = ['Adj Close ', 'Adj_Close', 'Close', 'Price', 'Last']
    target_col = next((c for c in possible_cols if c in df.columns), None)

    if not target_col:
        print(f"Skipping {filename}: Target column not found.")
        continue

    df = df.dropna(subset=[target_col])

    if 'Date' in df.columns:
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values('Date').reset_index(drop=True)

    data = df.filter([target_col]).values

    # --- B. NORMALIZE (0 to 1) ---
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(data)

    # --- C. CREATE SEQUENCES ---
    X, y = create_sequences(scaled_data, LOOKBACK)

    # Reshape for LSTM [Samples, Time Steps, Features]
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    # --- D. SPLIT ---
    train_size = int(len(X) * 0.8)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    # --- E. BUILD MODEL ---
    model = Sequential()

    # Layer 1
    model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
    model.add(Dropout(0.2))

    # Layer 2
    model.add(LSTM(units=50, return_sequences=False))
    model.add(Dropout(0.2))

    # Output Layer
    model.add(Dense(units=1))

    model.compile(optimizer='adam', loss='mean_squared_error')

    # --- F. TRAIN WITH CALLBACKS ---
    # Stop if validation loss doesn't improve for 10 epochs
    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(X_train, y_train,
                        batch_size=BATCH_SIZE,
                        epochs=EPOCHS,
                        validation_data=(X_test, y_test),
                        callbacks=[early_stop],
                        verbose=1)

    # --- G. PREDICT ---
    predictions = model.predict(X_test)

    # Inverse Transform (Get back to Rupees)
    predictions_rupees = scaler.inverse_transform(predictions)
    y_test_rupees = scaler.inverse_transform(y_test.reshape(-1, 1))

    # Calculate RMSE
    rmse = np.sqrt(np.mean(((predictions_rupees - y_test_rupees) ** 2)))

    # --- H. PLOTTING ---

    # PLOT 1: Price Prediction (Left)
    ax_price = axes[i, 0]
    plot_idx = np.arange(train_size + LOOKBACK, train_size + LOOKBACK + len(y_test))

    ax_price.plot(scaler.inverse_transform(scaled_data), color='gray', alpha=0.3, label='History')
    ax_price.plot(plot_idx, y_test_rupees, color='blue', linewidth=1.5, label='Actual (PKR)')
    ax_price.plot(plot_idx, predictions_rupees, color='red', linewidth=1.5, linestyle='--', label='LSTM Pred')

    ax_price.set_title(f"{filename} | RMSE: {rmse:.2f} PKR")
    ax_price.set_ylabel("Price (Rs.)")
    ax_price.legend()
    ax_price.grid(True, alpha=0.2)

    # PLOT 2: Loss Curve (Right)
    ax_loss = axes[i, 1]
    ax_loss.plot(history.history['loss'], label='Train Loss', color='orange')
    ax_loss.plot(history.history['val_loss'], label='Val Loss', color='purple')

    ax_loss.set_title(f"Training Dynamics (Stopped at Epoch {len(history.history['loss'])})")
    ax_loss.set_xlabel("Epoch")
    ax_loss.set_ylabel("Loss")
    ax_loss.legend()
    ax_loss.grid(True, alpha=0.2)

plt.tight_layout()
plt.show()